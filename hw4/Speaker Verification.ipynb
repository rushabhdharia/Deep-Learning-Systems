{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, Lambda, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow.keras.backend as K\n",
    "import librosa\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "from tensorflow.keras.metrics import BinaryAccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 16180)\n"
     ]
    }
   ],
   "source": [
    "with open('hw4_trs.pkl', 'rb') as pickle_file:\n",
    "    train_data = pickle.load(pickle_file)\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 22631)\n"
     ]
    }
   ],
   "source": [
    "with open('hw4_tes.pkl', 'rb') as pickle_file:\n",
    "    test_data = pickle.load(pickle_file)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Positive Pairs\n",
    "Generates all combinations of pairs and randomly selects L pairs from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pos_pairs(speaker, L=45):\n",
    "    batch = []\n",
    "    all_pairs = list(combinations(range(10), 2))\n",
    "    l_pairs = random.sample(all_pairs, L)\n",
    "    for a,b in l_pairs:\n",
    "        stft_1 = np.abs(librosa.stft(speaker[a], n_fft=1024, hop_length=512)).T\n",
    "        stft_2 = np.abs(librosa.stft(speaker[b], n_fft=1024, hop_length=512)).T\n",
    "        batch.append([stft_1, stft_2])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Negative Pairs\n",
    "The Main Speaker's utternaces lie between start and end. The other 49 speakers' utterances like before start and after end.  \n",
    "Randomly sample L utterances from the main speaker's utterances with replacement as L is greater than the number of utterances by the main speaker, i.e., L>10.  \n",
    "Whereas, we randomly sample L utterances from the other 49 speakers' utterances without replacement as L is less than their total utterances L<490.\n",
    "Using the samples from the above 2 steps we create Negative Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neg_pairs(pos_sp_num, train_data, L=45):\n",
    "    batch = []\n",
    "    \n",
    "    start = pos_sp_num*10\n",
    "    end = start + 10     \n",
    "    pos_spk = train_data[start: end]\n",
    "    neg_spk = train_data[:start] + train_data[end:]\n",
    "    neg_sample = random.sample(neg_spk, L)\n",
    "        \n",
    "    for l in range(L):\n",
    "        pos = random.choice(pos_spk)\n",
    "        \n",
    "        stft_pos = np.abs(librosa.stft(pos, n_fft=1024, hop_length=512)).T\n",
    "        stft_neg = np.abs(librosa.stft(neg_sample[l], n_fft=1024, hop_length=512)).T\n",
    "        batch.append([stft_pos, stft_neg])\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training Pairs\n",
    "Creates 2250 pairs of negative and positive examples each, i.e, 4500 total pairs.\n",
    "Final Dimensions - Pairs (4500) x Number of Inputs (2) x Shape of Spectrogram (45x513) x number of channels (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batches = []\n",
    "train_data = list(train_data)\n",
    "for i in range(50):\n",
    "    pos_batch = create_pos_pairs(train_data[i:i+10])\n",
    "    neg_batch = create_neg_pairs(i, train_data)\n",
    "    mini_batches += pos_batch + neg_batch\n",
    "#     mini_batches.append(mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_np = np.stack(mini_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4500, 2, 32, 513)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500, 2, 45, 513)\n"
     ]
    }
   ],
   "source": [
    "mini_np = np.pad(mini_np, ((0,0), (0,0), (0, 13), (0,0)), mode = 'constant', constant_values = 0) \n",
    "print(mini_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_np_exp = np.expand_dims(mini_np, axis = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4500, 2, 45, 513, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_np_exp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create True Predictions based on the negative/positive pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = [] \n",
    "for i in range(50):\n",
    "    y = np.zeros(90, dtype = int)\n",
    "    y[:45] += 1\n",
    "    Y.append(y)\n",
    "Y = np.hstack(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Testing Pairs\n",
    "Creates 900 pairs of negative and positive examples each, i.e, 1800 total pairs.  \n",
    "Final Dimensions - Pairs (1800) x Number of Inputs (2) x Shape of Spectrogram (45x513) x number of channels (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batches_test = []\n",
    "test_data = list(test_data)\n",
    "for i in range(20):\n",
    "    pos_batch = create_pos_pairs(test_data[i:i+10])\n",
    "    neg_batch = create_neg_pairs(i, test_data)\n",
    "    mini_batches_test += pos_batch + neg_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_np_test = np.stack(mini_batches_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 2, 45, 513)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_np_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 2, 45, 513, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_np_exp_test = np.expand_dims(mini_np_test, axis = 4)\n",
    "mini_np_exp_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "Binary Cross Entropy aka Sigmoid Cross Entropy is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_true, y_pred):\n",
    "    return tf.losses.sigmoid_cross_entropy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function copied from [1]\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    margin = 1\n",
    "    square_pred = K.square(y_pred)\n",
    "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
    "    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siamese_model(input_shape):\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    \n",
    "    # Base Network\n",
    "    base_model = Sequential()\n",
    "    base_model.add(Conv2D(32, kernel_size = (5, 5), input_shape = input_shape, activation = 'tanh'))\n",
    "    base_model.add(MaxPool2D())\n",
    "    base_model.add(Conv2D(64, kernel_size = (3, 3), activation = 'tanh'))\n",
    "    base_model.add(MaxPool2D())\n",
    "    base_model.add(Flatten())\n",
    "    base_model.add(Dense(1000, activation = 'tanh'))\n",
    "    \n",
    "    left_output = base_model(left_input)\n",
    "    right_output = base_model(right_input)\n",
    "    \n",
    "    prediction = K.dot(left_output, K.transpose(right_output))\n",
    "    \n",
    "    siamese_model = Model(inputs = [left_input, right_input], outputs = prediction)\n",
    "    \n",
    "    return siamese_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = siamese_model([45, 513, 1])\n",
    "model.compile(loss=contrastive_loss, optimizer = tf.keras.optimizers.Adam(), metrics=[BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 1800 samples\n",
      "Epoch 1/50\n",
      "4500/4500 [==============================] - 123s 27ms/sample - loss: 5363.9295 - binary_accuracy: 0.4654 - val_loss: 14.4255 - val_binary_accuracy: 0.5218\n",
      "Epoch 2/50\n",
      "4500/4500 [==============================] - 121s 27ms/sample - loss: 5.6737 - binary_accuracy: 0.4241 - val_loss: 11.4533 - val_binary_accuracy: 0.5222\n",
      "Epoch 3/50\n",
      "4500/4500 [==============================] - 112s 25ms/sample - loss: 2.1247 - binary_accuracy: 0.3855 - val_loss: 9.7710 - val_binary_accuracy: 0.5254\n",
      "Epoch 4/50\n",
      "4500/4500 [==============================] - 112s 25ms/sample - loss: 1.2082 - binary_accuracy: 0.3487 - val_loss: 9.5579 - val_binary_accuracy: 0.5254\n",
      "Epoch 5/50\n",
      "4500/4500 [==============================] - 112s 25ms/sample - loss: 0.7878 - binary_accuracy: 0.3209 - val_loss: 9.0885 - val_binary_accuracy: 0.5234\n",
      "Epoch 6/50\n",
      "4500/4500 [==============================] - 112s 25ms/sample - loss: 0.5926 - binary_accuracy: 0.3021 - val_loss: 8.8166 - val_binary_accuracy: 0.5228\n",
      "Epoch 7/50\n",
      "4500/4500 [==============================] - 112s 25ms/sample - loss: 0.4425 - binary_accuracy: 0.2917 - val_loss: 9.0638 - val_binary_accuracy: 0.5245\n",
      "Epoch 8/50\n",
      "4500/4500 [==============================] - 112s 25ms/sample - loss: 0.3565 - binary_accuracy: 0.2831 - val_loss: 9.0256 - val_binary_accuracy: 0.5227\n",
      "Epoch 9/50\n",
      "4500/4500 [==============================] - 112s 25ms/sample - loss: 0.3013 - binary_accuracy: 0.2792 - val_loss: 9.0249 - val_binary_accuracy: 0.5206\n",
      "Epoch 10/50\n",
      "4500/4500 [==============================] - 112s 25ms/sample - loss: 0.2727 - binary_accuracy: 0.2761 - val_loss: 9.1927 - val_binary_accuracy: 0.5239\n",
      "Epoch 11/50\n",
      "4500/4500 [==============================] - 112s 25ms/sample - loss: 0.2398 - binary_accuracy: 0.2720 - val_loss: 9.0533 - val_binary_accuracy: 0.5231\n",
      "Epoch 12/50\n",
      "4500/4500 [==============================] - 112s 25ms/sample - loss: 0.2185 - binary_accuracy: 0.2684 - val_loss: 8.9968 - val_binary_accuracy: 0.5215\n",
      "Epoch 13/50\n",
      "4500/4500 [==============================] - 115s 26ms/sample - loss: 0.2061 - binary_accuracy: 0.2652 - val_loss: 9.2377 - val_binary_accuracy: 0.5206\n",
      "Epoch 14/50\n",
      "4500/4500 [==============================] - 113s 25ms/sample - loss: 0.1935 - binary_accuracy: 0.2639 - val_loss: 9.3418 - val_binary_accuracy: 0.5211\n",
      "Epoch 15/50\n",
      "4500/4500 [==============================] - 113s 25ms/sample - loss: 0.1864 - binary_accuracy: 0.2615 - val_loss: 9.0081 - val_binary_accuracy: 0.5226\n",
      "Epoch 16/50\n",
      "4500/4500 [==============================] - 113s 25ms/sample - loss: 0.1808 - binary_accuracy: 0.2621 - val_loss: 8.9612 - val_binary_accuracy: 0.5196\n",
      "Epoch 17/50\n",
      "4500/4500 [==============================] - 112s 25ms/sample - loss: 0.1764 - binary_accuracy: 0.2604 - val_loss: 9.0531 - val_binary_accuracy: 0.5230\n",
      "Epoch 18/50\n",
      "4500/4500 [==============================] - 112s 25ms/sample - loss: 0.1718 - binary_accuracy: 0.2558 - val_loss: 9.1652 - val_binary_accuracy: 0.5200\n",
      "Epoch 19/50\n",
      "4500/4500 [==============================] - 112s 25ms/sample - loss: 0.1676 - binary_accuracy: 0.2551 - val_loss: 8.9305 - val_binary_accuracy: 0.5186\n",
      "Epoch 20/50\n",
      "4500/4500 [==============================] - 114s 25ms/sample - loss: 0.1644 - binary_accuracy: 0.2539 - val_loss: 8.7832 - val_binary_accuracy: 0.5183\n",
      "Epoch 21/50\n",
      "4500/4500 [==============================] - 158s 35ms/sample - loss: 0.1637 - binary_accuracy: 0.2558 - val_loss: 9.2363 - val_binary_accuracy: 0.5193\n",
      "Epoch 22/50\n",
      "4500/4500 [==============================] - 164s 36ms/sample - loss: 0.1632 - binary_accuracy: 0.2560 - val_loss: 8.8239 - val_binary_accuracy: 0.5170\n",
      "Epoch 23/50\n",
      "4500/4500 [==============================] - 164s 37ms/sample - loss: 0.1600 - binary_accuracy: 0.2511 - val_loss: 9.2202 - val_binary_accuracy: 0.5185\n",
      "Epoch 24/50\n",
      "4500/4500 [==============================] - 164s 36ms/sample - loss: 0.1577 - binary_accuracy: 0.2496 - val_loss: 8.8814 - val_binary_accuracy: 0.5148\n",
      "Epoch 25/50\n",
      "4500/4500 [==============================] - 167s 37ms/sample - loss: 0.1583 - binary_accuracy: 0.2525 - val_loss: 9.1254 - val_binary_accuracy: 0.5152\n",
      "Epoch 26/50\n",
      "4500/4500 [==============================] - 164s 36ms/sample - loss: 0.1582 - binary_accuracy: 0.2523 - val_loss: 8.7457 - val_binary_accuracy: 0.5180\n",
      "Epoch 27/50\n",
      "1184/4500 [======>.......................] - ETA: 1:50 - loss: 0.1614 - binary_accuracy: 0.2534"
     ]
    }
   ],
   "source": [
    "model.fit([mini_np_exp[:, 0], mini_np_exp[:, 1]], Y, batch_size=32, epochs=50, validation_data=([mini_np_exp_test[:, 0], mini_np_exp_test[:, 1]] ,Y[:1800]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. https://keras.io/examples/mnist_siamese/\n",
    "2. https://medium.com/predict/face-recognition-from-scratch-using-siamese-networks-and-tensorflow-df03e32f8cd0\n",
    "3. https://becominghuman.ai/siamese-networks-algorithm-applications-and-pytorch-implementation-4ffa3304c18\n",
    "4. https://towardsdatascience.com/one-shot-learning-with-siamese-networks-using-keras-17f34e75bb3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('siamese.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = model.load('siamese.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
