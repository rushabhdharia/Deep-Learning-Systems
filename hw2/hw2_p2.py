# -*- coding: utf-8 -*-
"""hw2_p2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QgkqUWSPUGtaHmzanb2E2pTEY656YMdV

# Speech Denoising using 2d CNN
"""

import librosa
import numpy as np
import tensorflow as tf

from google.colab import files

uploaded = files.upload()

s, sr = librosa.load('data/train_clean_male.wav', sr = None)
S_clean = librosa.stft(s, n_fft=1024, hop_length=512)
S_mag = np.abs(S_clean).T

sn, sr = librosa.load('data/train_dirty_male.wav', sr = None)
X_dirty = librosa.stft(s, n_fft=1024, hop_length=512)
X_mag = np.abs(X_dirty).T

def create_placeholders(n_x = 513, n_y = 513):
  X = tf.placeholder(tf.float32, [None, 20, n_x])
  Y = tf.placeholder(tf.float32, [None, n_y])
  return X, Y

def conv2d(x, W, b, stride = 1):
  x = tf.nn.conv2d(x, W, strides = [1, stride, stride, 1], padding = 'VALID')
  x = tf.nn.bias_add(x, b)
  return tf.nn.relu(x)

def max_pool2d(x, k = 2):
  return tf.nn.max_pool2d(x, ksize = [1, k, k, 1], strides = [1, k, k, 1], padding = 'VALID')

def initialize_parameters():
  W1 = tf.get_variable("W1", [3, 3, 1, 16], initializer=tf.contrib.layers.variance_scaling_initializer())
  b1 = tf.get_variable("b1", [16], initializer=tf.zeros_initializer)
 
  W2 = tf.get_variable("W2", [5, 5, 16, 32], initializer=tf.contrib.layers.variance_scaling_initializer())
  b2 = tf.get_variable("b2", [32], initializer=tf.zeros_initializer)
  
#   W3 = tf.get_variable("W3", [7, 7, 32, 64], initializer=tf.contrib.layers.variance_scaling_initializer())
#   b3 = tf.get_variable("b3", [64], initializer=tf.zeros_initializer)
  
  W4 = tf.get_variable("W4", [513, 250], initializer=tf.contrib.layers.variance_scaling_initializer())
  b4 = tf.get_variable("b4", [513, 1], initializer=tf.zeros_initializer) 

#   W5 = tf.get_variable("W5", [513, 1024], initializer=tf.contrib.layers.variance_scaling_initializer())
#   b5 = tf.get_variable("b5", [513, 1], initializer=tf.zeros_initializer)
  

  parameters = {
      "W1" : W1,
      "b1" : b1,
      "W2" : W2,
      "b2" : b2,
#       "W3" : W3,
#       "b3" : b3,
      "W4" : W4,
      "b4" : b4,
#       "W5" : W5,
#       "b5" : b5
  }
  
  return parameters

def forward_propagation(X, parameters):
  print("________________________________________")
#   print(X.shape)
  X = tf.reshape(X, [-1, 20, 513, 1])
#   print(X.shape)
  
  W1 = parameters['W1']
  b1 = parameters['b1']
  A1 = conv2d(X, W1, b1, stride = 1)
#   print(A1.shape)
  A2 = max_pool2d(A1, k=2)
#   print(A2.shape)
  
  W2 = parameters['W2']
  b2 = parameters['b2']
  A3 = conv2d(A2, W2, b2, stride = 1)
#   print(A3.shape)
  A4 = max_pool2d(A3, k=2)
#   print(A4.shape)
  
#   W3 = parameters['W3']
#   b3 = parameters['b3']
#   A5 = conv2d(A4, W3, b3, stride = 1)
#   A6 = max_pool2d(A5, k=2)
#   print("A5 = ", A5.shape)
#   print("A6 = ", A6.shape)  
  
  A6 = tf.transpose(tf.reshape(A4, [-1, A4.shape[1] * A4.shape[2]]))
  
#   print("Flatten = ", A6.shape)
  
  W4 = parameters['W4']
  b4 = parameters['b4']
#   print(W4.shape)
#   print(A6.shape)
#   print(b4.shape)
  Z7 = tf.matmul(W4, A6) + b4
#   A7 = tf.nn.relu(Z7)
#   print(A7.shape)
  
#   W5 = parameters['W5']
#   b5 = parameters['b5']
#   Z8 = tf.matmul(W5, A7) + b5
#   print("Z8 = ", Z8.shape)
  print("________________________________________")
  return tf.transpose(Z7)


def create_batches(input):
#   num_batches = input.shape[0]//20
#   print(input.shape[0])
  batches = []
  start = 0
  while start<input.shape[0]-19:
    end = start+20
    batches.append(input[start:end, :])
    start += 1
    
#   print(end)
    
  return batches

X_batch = create_batches(X_mag)

len(X_batch)

X_batch = np.array(X_batch)

X_batch.shape

def compute_cost(Z8, Y):
  tf.print("Z8 = ", Z8.shape)
  tf.print("Y = ", Y.shape)
  cost = tf.reduce_mean(tf.losses.mean_squared_error(tf.nn.relu(Z8), Y))
  
  return cost

def model(X_mag, S_mag, learning_rate = 0.0001, epochs = 500, print_cost = True, minibatch_size = 32):
  
  tf.reset_default_graph()
  m = X_mag.shape[0]
  X, Y = create_placeholders()
#   print("xshape=", X.shape)
#   print("yshape = ", Y.shape)
  parameters = initialize_parameters()
  
  Z8 = forward_propagation(X, parameters)

  cost = compute_cost(Z8, Y)
  
  optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)
  init = tf.global_variables_initializer()
  
  with tf.Session() as sess:
    sess.run(init)
    
    costs = []
    
    for epoch in range(epochs):
      epoch_cost = 0.
      
      num_minibatches = m//minibatch_size
      
      for i in range(num_minibatches):
        start = i*minibatch_size
        end = start + minibatch_size
        
        mini_X = X_mag[start:end, :, :]        
        mini_Y = S_mag[start:end, :]
#         print(mini_X.shape, mini_Y.shape)
        
        _, minibatch_cost = sess.run([optimizer, cost], feed_dict = {X: mini_X, Y:mini_Y})
#         print("here")
        epoch_cost += minibatch_cost / num_minibatches
                               
      if print_cost == True and epoch % 100 == 0:
        print("Cost after epoch %i: %f" %(epoch, epoch_cost))
        
      if print_cost == True and epoch % 5 == 0:
        costs.append(epoch_cost)
        
    parameters = sess.run(parameters)
    print ("Parameters have been trained!")

    sess.close()
    return parameters

parameters = model(X_batch, S_mag[19:])

