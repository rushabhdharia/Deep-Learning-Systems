{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, Lambda, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow.keras.backend as K\n",
    "import librosa\n",
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hw4_trs.pkl', 'rb') as pickle_file:\n",
    "    train_data = pickle.load(pickle_file)\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hw4_tes.pkl', 'rb') as pickle_file:\n",
    "    test_data = pickle.load(pickle_file)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pos_pairs(speaker, L=45):\n",
    "    batch = []\n",
    "    all_pairs = list(combinations(range(10), 2))\n",
    "    l_pairs = random.sample(all_pairs, L)\n",
    "    for a,b in l_pairs:\n",
    "        stft_1 = np.abs(librosa.stft(speaker[a], n_fft=1024, hop_length=512)).T\n",
    "        stft_2 = np.abs(librosa.stft(speaker[b], n_fft=1024, hop_length=512)).T\n",
    "        batch.append([stft_1, stft_2])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neg_pairs(pos_sp_num, train_data, L=45):\n",
    "    batch = []\n",
    "    \n",
    "    start = pos_sp_num*10\n",
    "    end = start + 10     \n",
    "    pos_spk = train_data[start: end]\n",
    "    neg_spk = train_data[:start] + train_data[end:]\n",
    "    neg_sample = random.sample(neg_spk, L)\n",
    "        \n",
    "    for l in range(L):\n",
    "        pos = random.choice(pos_spk)\n",
    "        \n",
    "        stft_pos = np.abs(librosa.stft(pos, n_fft=1024, hop_length=512)).T\n",
    "        stft_neg = np.abs(librosa.stft(neg_sample[l], n_fft=1024, hop_length=512)).T\n",
    "        batch.append([stft_pos, stft_neg])\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batches = []\n",
    "train_data = list(train_data)\n",
    "for i in range(50):\n",
    "    pos_batch = create_pos_pairs(train_data[i:i+10])\n",
    "    neg_batch = create_neg_pairs(i, train_data)\n",
    "    mini_batches += pos_batch + neg_batch\n",
    "#     mini_batches.append(mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_np = np.stack(mini_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4500, 2, 32, 513)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function copied from [1]\n",
    "def euclidean_dist(vects):\n",
    "    x, y = vects\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function copied from [1]\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    margin = 1\n",
    "    square_pred = K.square(y_pred)\n",
    "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
    "    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siamese_model(input_shape):\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    \n",
    "    # Base Network\n",
    "    base_model = Sequential()\n",
    "    base_model.add(Conv2D(32, kernel_size = (5, 5), input_shape = input_shape, activation = 'relu'))\n",
    "    base_model.add(MaxPool2D())\n",
    "    base_model.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu'))\n",
    "    base_model.add(MaxPool2D())\n",
    "    base_model.add(Flatten())\n",
    "    base_model.add(Dense(1000, activation = 'sigmoid'))\n",
    "    \n",
    "    left_output = base_model(left_input)\n",
    "    right_output = base_model(right_input)\n",
    "   \n",
    "    L2_distance = Lambda(euclidean_dist, output_shape=(1000, 1))([left_output, right_output])\n",
    "    \n",
    "    prediction = Dense(1, activation='sigmoid')(L2_distance)\n",
    "    \n",
    "    siamese_model = Model(inputs = [left_input, right_input], outputs = prediction)\n",
    "    \n",
    "    return siamese_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = siamese_model([32, 513, 1])\n",
    "model.compile(loss=contrastive_loss, optimizer = tf.keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = [] \n",
    "for i in range(50):\n",
    "    y = np.zeros(90, dtype = int)\n",
    "    y[:45] += 1\n",
    "    Y.append(y)\n",
    "Y = np.hstack(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_np_exp = np.expand_dims(mini_np, axis = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4500, 2, 32, 513, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_np_exp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4500/4500 [==============================] - 133s 29ms/sample - loss: 0.1982\n",
      "Epoch 2/50\n",
      "4500/4500 [==============================] - 138s 31ms/sample - loss: 0.1743\n",
      "Epoch 3/50\n",
      "4500/4500 [==============================] - 135s 30ms/sample - loss: 0.1618\n",
      "Epoch 4/50\n",
      "4500/4500 [==============================] - 121s 27ms/sample - loss: 0.1512\n",
      "Epoch 5/50\n",
      "4500/4500 [==============================] - 135s 30ms/sample - loss: 0.1456\n",
      "Epoch 6/50\n",
      "4500/4500 [==============================] - 140s 31ms/sample - loss: 0.1410\n",
      "Epoch 7/50\n",
      "4500/4500 [==============================] - 135s 30ms/sample - loss: 0.1368\n",
      "Epoch 8/50\n",
      "4500/4500 [==============================] - 128s 29ms/sample - loss: 0.1330\n",
      "Epoch 9/50\n",
      "4500/4500 [==============================] - 114s 25ms/sample - loss: 0.1293\n",
      "Epoch 10/50\n",
      "4500/4500 [==============================] - 52s 12ms/sample - loss: 0.1259\n",
      "Epoch 11/50\n",
      "4500/4500 [==============================] - 52s 11ms/sample - loss: 0.1227\n",
      "Epoch 12/50\n",
      "4500/4500 [==============================] - 52s 12ms/sample - loss: 0.1197\n",
      "Epoch 13/50\n",
      "4500/4500 [==============================] - 52s 12ms/sample - loss: 0.1169\n",
      "Epoch 14/50\n",
      "4500/4500 [==============================] - 52s 12ms/sample - loss: 0.1142\n",
      "Epoch 15/50\n",
      "4500/4500 [==============================] - 53s 12ms/sample - loss: 0.1118\n",
      "Epoch 16/50\n",
      "4500/4500 [==============================] - 53s 12ms/sample - loss: 0.1094\n",
      "Epoch 17/50\n",
      "4500/4500 [==============================] - 52s 12ms/sample - loss: 0.1073\n",
      "Epoch 18/50\n",
      "4500/4500 [==============================] - 52s 12ms/sample - loss: 0.1052\n",
      "Epoch 19/50\n",
      "4500/4500 [==============================] - 52s 12ms/sample - loss: 0.1033\n",
      "Epoch 20/50\n",
      "4500/4500 [==============================] - 51s 11ms/sample - loss: 0.1015\n",
      "Epoch 21/50\n",
      "4500/4500 [==============================] - 52s 12ms/sample - loss: 0.0998\n",
      "Epoch 22/50\n",
      "4500/4500 [==============================] - 54s 12ms/sample - loss: 0.0982\n",
      "Epoch 23/50\n",
      "4500/4500 [==============================] - 53s 12ms/sample - loss: 0.0968\n",
      "Epoch 24/50\n",
      "4500/4500 [==============================] - 53s 12ms/sample - loss: 0.0954\n",
      "Epoch 25/50\n",
      "4500/4500 [==============================] - 51s 11ms/sample - loss: 0.0942\n",
      "Epoch 26/50\n",
      "4500/4500 [==============================] - 51s 11ms/sample - loss: 0.0941\n",
      "Epoch 27/50\n",
      "4500/4500 [==============================] - 51s 11ms/sample - loss: 0.1209\n",
      "Epoch 28/50\n",
      "4500/4500 [==============================] - 51s 11ms/sample - loss: 0.0938\n",
      "Epoch 29/50\n",
      "4500/4500 [==============================] - 51s 11ms/sample - loss: 0.0904\n",
      "Epoch 30/50\n",
      "4500/4500 [==============================] - 52s 12ms/sample - loss: 0.0889\n",
      "Epoch 31/50\n",
      "4500/4500 [==============================] - 51s 11ms/sample - loss: 0.0876\n",
      "Epoch 32/50\n",
      "4500/4500 [==============================] - 51s 11ms/sample - loss: 0.0865\n",
      "Epoch 33/50\n",
      "4500/4500 [==============================] - 51s 11ms/sample - loss: 0.0854\n",
      "Epoch 34/50\n",
      "4500/4500 [==============================] - 51s 11ms/sample - loss: 0.0844\n",
      "Epoch 35/50\n",
      "4500/4500 [==============================] - 51s 11ms/sample - loss: 0.0835\n",
      "Epoch 36/50\n",
      "4500/4500 [==============================] - 51s 11ms/sample - loss: 0.0826\n",
      "Epoch 37/50\n",
      "4500/4500 [==============================] - 51s 11ms/sample - loss: 0.0818\n",
      "Epoch 38/50\n",
      "4500/4500 [==============================] - 51s 11ms/sample - loss: 0.0810\n",
      "Epoch 39/50\n",
      "4500/4500 [==============================] - 51s 11ms/sample - loss: 0.0802\n",
      "Epoch 40/50\n",
      "4500/4500 [==============================] - 51s 11ms/sample - loss: 0.0795\n",
      "Epoch 41/50\n",
      "4500/4500 [==============================] - 51s 11ms/sample - loss: 0.0789\n",
      "Epoch 42/50\n",
      "4500/4500 [==============================] - 51s 11ms/sample - loss: 0.0783\n",
      "Epoch 43/50\n",
      "4500/4500 [==============================] - 51s 11ms/sample - loss: 0.0776\n",
      "Epoch 44/50\n",
      "4500/4500 [==============================] - 52s 12ms/sample - loss: 0.0770\n",
      "Epoch 45/50\n",
      "4500/4500 [==============================] - 53s 12ms/sample - loss: 0.0765\n",
      "Epoch 46/50\n",
      "4500/4500 [==============================] - 52s 11ms/sample - loss: 0.0759\n",
      "Epoch 47/50\n",
      "4500/4500 [==============================] - 51s 11ms/sample - loss: 0.0754\n",
      "Epoch 48/50\n",
      "4500/4500 [==============================] - 51s 11ms/sample - loss: 0.0749\n",
      "Epoch 49/50\n",
      "4500/4500 [==============================] - 52s 12ms/sample - loss: 0.0745\n",
      "Epoch 50/50\n",
      "4500/4500 [==============================] - 52s 12ms/sample - loss: 0.0740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f49c80e7438>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([mini_np_exp[:, 0], mini_np_exp[:, 1]], Y, batch_size=90, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batches_test = []\n",
    "test_data = list(test_data)\n",
    "for i in range(20):\n",
    "    pos_batch = create_pos_pairs(test_data[i:i+10])\n",
    "    neg_batch = create_neg_pairs(i, test_data)\n",
    "    mini_batches_test += pos_batch + neg_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_np_test = np.stack(mini_batches_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 2, 45, 513)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_np_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 2, 45, 513, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_np_exp_test = np.expand_dims(mini_np_test, axis = 4)\n",
    "mini_np_exp_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800/1800 [==============================] - 7s 4ms/sample - loss: 0.3677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.36768981088987657"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([mini_np_exp_test[:, 0, :32], mini_np_exp_test[:, 1, 2:34]] ,Y[:1800], batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. https://keras.io/examples/mnist_siamese/\n",
    "2. https://medium.com/predict/face-recognition-from-scratch-using-siamese-networks-and-tensorflow-df03e32f8cd0\n",
    "3. https://becominghuman.ai/siamese-networks-algorithm-applications-and-pytorch-implementation-4ffa3304c18\n",
    "4. https://towardsdatascience.com/one-shot-learning-with-siamese-networks-using-keras-17f34e75bb3d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
